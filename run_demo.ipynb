{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cd4a06-fd78-4a8a-bfe8-16e4833c1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,sys,logging,os,re,subprocess\n",
    "from typing import Optional, List, Tuple, Mapping\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from onnxruntime import InferenceSession, SessionOptions, GraphOptimizationLevel\n",
    "import concurrent.futures\n",
    "\n",
    "os.environ[\"HF_OFFLINE\"] = \"1\"\n",
    "\n",
    "def resize(pic: Image.Image, size: int, keep_ratio: float = True) -> Image.Image:\n",
    "    \"\"\"按指定要求调整图像的大小\"\"\"\n",
    "    if not keep_ratio:\n",
    "        target_size = (size, size)\n",
    "    else:\n",
    "        min_edge = min(pic.size)\n",
    "        target_size = (int(pic.size[0] / min_edge * size), int(pic.size[1] / min_edge * size))\n",
    "    target_size = ((target_size[0] // 4) * 4, (target_size[1] // 4) * 4)\n",
    "    return pic.resize(target_size, resample=Image.Resampling.BILINEAR)\n",
    "\n",
    "def to_tensor(pic: Image.Image):\n",
    "    \"\"\"张量,调整和归一化\"\"\"\n",
    "    img: np.ndarray = np.array(pic, np.uint8, copy=True)\n",
    "    img = img.reshape(pic.size[1], pic.size[0], len(pic.getbands()))\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    return img.astype(np.float32) / 255\n",
    "\n",
    "def fill_background(pic: Image.Image, background: str = 'white') -> Image.Image:\n",
    "    \"\"\"颜色处理\"\"\"\n",
    "    if pic.mode == 'RGB':\n",
    "        return pic\n",
    "    if pic.mode != 'RGBA':\n",
    "        pic = pic.convert('RGBA')\n",
    "    background = background or 'white'\n",
    "    result = Image.new('RGBA', pic.size, background)\n",
    "    result.paste(pic, (0, 0), pic)\n",
    "    return result.convert('RGB')\n",
    "\n",
    "def image_to_tensor(pic: Image.Image, size: int = 512, keep_ratio: float = True, background: str = 'white'):\n",
    "    return to_tensor(resize(fill_background(pic, background), size, keep_ratio))\n",
    "\n",
    "def open_onnx_model(ckpt: str, provider: str) -> InferenceSession:\n",
    "    options = SessionOptions()\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    logging.info(f'Model {ckpt!r} loaded with provider {provider!r}')\n",
    "    return InferenceSession(ckpt, options, [provider])\n",
    "\n",
    "def load_classes(onnx_model_path) -> List[str]:\n",
    "    classes_file = os.path.join(onnx_model_path, 'classes.json')\n",
    "    with open(classes_file, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_tags_from_image( pic: Image.Image, onnx_model_path, model, threshold: float = 0.7, size: int = 512, keep_ratio: bool = False):\n",
    "    real_input = image_to_tensor(pic, size, keep_ratio)\n",
    "    real_input = real_input.reshape(1, *real_input.shape)\n",
    "\n",
    "    native_output, = model.run(['output'], {'input': real_input})\n",
    "\n",
    "    output = (1 / (1 + np.exp(-native_output))).reshape(-1)\n",
    "    tags = load_classes(onnx_model_path)\n",
    "    pairs = sorted([(tags[i], ratio) for i, ratio in enumerate(output)], key=lambda x: (-x[1], x[0]))\n",
    "    del real_input, native_output, output\n",
    "    return {tag: float(ratio) for tag, ratio in pairs if ratio >= threshold}\n",
    "\n",
    "def image_to_mldanbooru_tags(filtered_tags, use_spaces: bool, use_escape: bool, include_ranks: bool, score_descend: bool) \\\n",
    "        -> Tuple[str, Mapping[str, float]]:\n",
    "    text_items = []\n",
    "    tags_pairs = filtered_tags.items()\n",
    "    if score_descend:\n",
    "        tags_pairs = sorted(tags_pairs, key=lambda x: (-x[1], x[0]))\n",
    "    for tag, score in tags_pairs:\n",
    "        tag_outformat = tag\n",
    "        if use_spaces:\n",
    "            tag_outformat = tag_outformat.replace('_', ' ')\n",
    "        if use_escape:\n",
    "            RE_SPECIAL = re.compile(r'([\\\\()])')\n",
    "            tag_outformat = re.sub(RE_SPECIAL, r'\\\\\\1', tag_outformat)\n",
    "        if include_ranks:\n",
    "            tag_outformat = f\"({tag_outformat}:{score:.3f})\"\n",
    "        text_items.append(tag_outformat)\n",
    "    output_text = ', '.join(text_items)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "def process_main(image_path, onnx_model_path, model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend):\n",
    "    input_image = Image.open(image_path)\n",
    "    filtered_tags = get_tags_from_image(input_image, onnx_model_path, model, threshold, size, keep_ratio)\n",
    "    result_text = image_to_mldanbooru_tags(filtered_tags, use_spaces, use_escape, include_ranks, score_descend)\n",
    "    del image_path, onnx_model_path, model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend, input_image, filtered_tags\n",
    "    return result_text\n",
    "\n",
    "def process_image(image_path, onnx_model_path, model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend, output_path=None, extension=\"ml_danbooru\"):\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    if output_path=None:\n",
    "        output_file_path = os.path.join(os.path.dirname(image_path), f\"{base_name}.{extension}\")\n",
    "    else:\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        output_file_path = os.path.join(os.path.dirname(image_path), f\"{output_path}.{extension}\")\n",
    "    result_text = main(image_path, onnx_model_path, model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend)\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(result_text)\n",
    "\n",
    "def process_images(image_paths, onnx_model_path, model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend, output_path=None, batch_size=16, extension=\"ml_danbooru\"):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_futures = {executor.submit(process_image, path, onnx_model_path, model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend): path for path in batch_paths}\n",
    "            futures.extend(batch_futures)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {e}\")\n",
    "\n",
    "def get_image_paths(temporarily_dir, train_data_dir):\n",
    "    if os.path.isdir(train_data_dir):\n",
    "        zip_files = [file for file in os.listdir(train_data_dir) if file.endswith('.zip')]\n",
    "        if len(zip_files) == 1:\n",
    "            zip_file_path = os.path.join(train_data_dir, zip_files[0])\n",
    "        else:\n",
    "            print(\"zip文件必须有且仅能有一个\")\n",
    "            exit()\n",
    "    if train_data_dir.endswith('.zip'):\n",
    "        zip_file_path = train_data_dir\n",
    "    os.makedirs(temporarily_dir, exist_ok=True)\n",
    "    unzip_zipfile(zip_file_path, temporarily_dir, password=\"shiertier\")\n",
    "    base_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n",
    "    IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\", \".BMP\"]\n",
    "    pic_dir = os.path.join(temporarily_dir, base_name)\n",
    "    image_files = [f for f in os.listdir(pic_dir) if f.endswith(tuple(IMAGE_EXTENSIONS))]\n",
    "    image_paths = [os.path.join(pic_dir, f) for f in os.listdir(pic_dir) if f.endswith(tuple(IMAGE_EXTENSIONS))]\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    temporarily_dir = args.temporarily_dir\n",
    "    train_data_dir = args.train_data_dir\n",
    "    pic_dir = args.pic_path\n",
    "    onnx_model_path = args.onnx_model_path\n",
    "    onnx_model_name = args.onnx_model_name\n",
    "    output_path = args.output_path\n",
    "    batch_size = args.batch_size\n",
    "    extension = args.extension\n",
    "    threshold = args.threshold\n",
    "    size = args.size\n",
    "    keep_ratio = args.keep_ratio\n",
    "    use_spaces = args.use_spaces\n",
    "    use_escape = args.use_escape\n",
    "    include_ranks = args.include_ranks\n",
    "    score_descend = args.score_descend\n",
    "\n",
    "    if onnx_model_path is None：\n",
    "        print(\"请先下载仓库https://huggingface.co/deepghs/ml-danbooru-onnx，并指定存储目录为onnx_model_path\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if train_data_dir is not None and pic_path is not None:\n",
    "        print(\"错误：不能同时指定 --train_data_dir 和 --pic_path。\")\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    if train_data_dir is None and pic_path is None:\n",
    "        print(\"错误：--train_data_dir 和 --pic_path 至少需要指定一个。\")\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    if train_data_dir is not None:\n",
    "        if temporarily_dir is None:\n",
    "            temporarily_dir = os.path.join(os.path.expanduser(\"~\"), \"tarin\")\n",
    "            os.makedirs(temporarily_dir, exist_ok=True)\n",
    "        image_paths = get_image_paths(temporarily_dir, train_data_dir)\n",
    "        ml_model = open_onnx_model(os.path.join(onnx_model_path, onnx_model_name), \"CUDAExecutionProvider\")\n",
    "        process_images(image_paths, onnx_model_path, ml_model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend, output_path, batch_size, extension)\n",
    "\n",
    "    if pic_dir is not None:\n",
    "        ml_model = open_onnx_model(os.path.join(onnx_model_path, onnx_model_name), \"CUDAExecutionProvider\")\n",
    "        process_image(pic_dir, onnx_model_path, ml_model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend, output_path, extension)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"程序描述\")\n",
    "    parser.add_argument(\"--temporarily_dir\", default=None, help=\"指定临时目录，当使用压缩包时，图片将解压到这里\")\n",
    "    parser.add_argument(\"--train_data_dir\", default=None, help=\"指定训练数据目录或目录内的zip文件\")\n",
    "    parser.add_argument(\"--pic_path\", default=None, help=\"指定单张图片路径\")\n",
    "    parser.add_argument(\"--onnx_model_path\", default=None, help=\"指定ONNX模型路径\")\n",
    "    parser.add_argument(\"--onnx_model_name\", default=\"ml_caformer_m36_dec-5-97527.onnx\", help=\"指定ONNX模型名称\")\n",
    "    parser.add_argument(\"--output_path\", default=None, help=\"指定输出路径\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"指定批处理大小\")\n",
    "    parser.add_argument(\"--extension\", default=\"ml_danbooru\", help=\"指定文件扩展名\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=0.68, help=\"指定阈值\")\n",
    "    parser.add_argument(\"--size\", type=int, default=512， help=\"指定图片大小\")\n",
    "    parser.add_argument(\"--keep_ratio\", default=True, action=\"store_true\", help=\"保持图片比例\")\n",
    "    parser.add_argument(\"--use_spaces\", default=True, action=\"store_true\", help=\"使用空格替代下划线\")\n",
    "    parser.add_argument(\"--use_escape\", default=False, action=\"store_true\", help=\"在标签中转义斜杠和括号\")\n",
    "    parser.add_argument(\"--include_ranks\", default=False, action=\"store_true\", help=\"在输出文本中包含排名\")\n",
    "    parser.add_argument(\"--score_descend\", default=True, action=\"store_true\", help=\"按分数降序排列标签\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbe0cd-6500-48e6-9331-4313a5ab61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    temporarily_dir = args.temporarily_dir\n",
    "    train_data_dir = args.train_data_dir\n",
    "    pic_dir = args.pic_path\n",
    "    onnx_model_path = args.onnx_model_path\n",
    "    onnx_model_name = args.onnx_model_name\n",
    "    output_path = args.output_path\n",
    "    batch_size = args.batch_size\n",
    "    extension = args.extension\n",
    "    threshold = args.threshold\n",
    "    size = args.size\n",
    "    keep_ratio = args.keep_ratio\n",
    "    use_spaces = args.use_spaces\n",
    "    use_escape = args.use_escape\n",
    "    include_ranks = args.include_ranks\n",
    "    score_descend = args.score_descend\n",
    "\n",
    "    if onnx_model_path is None：\n",
    "        print(\"请先下载仓库https://huggingface.co/deepghs/ml-danbooru-onnx，并指定存储目录为onnx_model_path\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if train_data_dir is not None and pic_path is not None:\n",
    "        print(\"错误：不能同时指定 --train_data_dir 和 --pic_path。\")\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    if train_data_dir is None and pic_path is None:\n",
    "        print(\"错误：--train_data_dir 和 --pic_path 至少需要指定一个。\")\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    if train_data_dir is not None:\n",
    "        if temporarily_dir is None:\n",
    "            temporarily_dir = os.path.join(os.path.expanduser(\"~\"), \"tarin\")\n",
    "            os.makedirs(temporarily_dir, exist_ok=True)\n",
    "        image_paths = get_image_paths(temporarily_dir, train_data_dir)\n",
    "        ml_model = open_onnx_model(os.path.join(onnx_model_path, onnx_model_name), \"CUDAExecutionProvider\")\n",
    "        process_images(image_paths, onnx_model_path, ml_model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend, output_path, batch_size, extension)\n",
    "\n",
    "    if pic_dir is not None:\n",
    "        ml_model = open_onnx_model(os.path.join(onnx_model_path, onnx_model_name), \"CUDAExecutionProvider\")\n",
    "        process_image(pic_dir, onnx_model_path, ml_model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend, output_path, extension)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"程序描述\")\n",
    "    parser.add_argument(\"--temporarily_dir\", default=None, help=\"指定临时目录，当使用压缩包时，图片将解压到这里\")\n",
    "    parser.add_argument(\"--train_data_dir\", default=None, help=\"指定训练数据目录或目录内的zip文件\")\n",
    "    parser.add_argument(\"--pic_path\", default=None, help=\"指定单张图片路径\")\n",
    "    parser.add_argument(\"--onnx_model_path\", default=None, help=\"指定ONNX模型路径\")\n",
    "    parser.add_argument(\"--onnx_model_name\", default=\"ml_caformer_m36_dec-5-97527.onnx\", help=\"指定ONNX模型名称\")\n",
    "    parser.add_argument(\"--output_path\", default=None, help=\"指定输出路径\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"指定批处理大小\")\n",
    "    parser.add_argument(\"--extension\", default=\"ml_danbooru\", help=\"指定文件扩展名\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=0.68, help=\"指定阈值\")\n",
    "    parser.add_argument(\"--size\", type=int, default=512， help=\"指定图片大小\")\n",
    "    parser.add_argument(\"--keep_ratio\", default=True, action=\"store_true\", help=\"保持图片比例\")\n",
    "    parser.add_argument(\"--use_spaces\", default=True, action=\"store_true\", help=\"使用空格替代下划线\")\n",
    "    parser.add_argument(\"--use_escape\", default=False, action=\"store_true\", help=\"在标签中转义斜杠和括号\")\n",
    "    parser.add_argument(\"--include_ranks\", default=False, action=\"store_true\", help=\"在输出文本中包含排名\")\n",
    "    parser.add_argument(\"--score_descend\", default=True, action=\"store_true\", help=\"按分数降序排列标签\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a7557-6b4a-40c4-952f-152610646ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\", ...]\n",
    "onnx_model_path = \"/models/ML-Danbooru\"\n",
    "onnx_model_name = \"ml_caformer_m36_dec-5-97527.onnx\"\n",
    "ml_model = open_onnx_model(os.path.join(onnx_model_path, onnx_model_name), \"CUDAExecutionProvider\")\n",
    "threshold = 0.7\n",
    "size = 960 #图片短边长\n",
    "keep_ratio = True #保持比例\n",
    "use_spaces = False #使用空格替换下划线\n",
    "use_escape = True #对标签中的反斜杠和括号进行转义\n",
    "include_ranks = False #此选项将在输出文本中为每个标签添加排名或分数。排名格式为\"tag:score\"，并用括号括起来。\n",
    "score_descend = True #根据其分数的排序顺序\n",
    "print(\"加载ml模型\")\n",
    "ml_model = open_onnx_model(os.path.join(onnx_model_path, onnx_model_name), \"CUDAExecutionProvider\")\n",
    "print(\"ml模型加载成功\")\n",
    "process_images(image_paths, onnx_model_path, ml_model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c67c4-e77d-42bb-b2d0-550b3acecb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载ML-Danbooru模型\n",
    "onnx_model_path = \"/models/ML-Danbooru\"\n",
    "onnx_model_name = \"ml_caformer_m36_dec-5-97527.onnx\"\n",
    "threshold = 0.7\n",
    "size = 960 #图片短边长\n",
    "keep_ratio = True #保持比例\n",
    "use_spaces = False #使用空格替换下划线\n",
    "use_escape = True #对标签中的反斜杠和括号进行转义\n",
    "include_ranks = False #此选项将在输出文本中为每个标签添加排名或分数。排名格式为\"tag:score\"，并用括号括起来。\n",
    "score_descend = True #根据其分数的排序顺序\n",
    "print(\"加载ml模型\")\n",
    "ml_model = open_onnx_model(os.path.join(onnx_model_path, onnx_model_name), \"CUDAExecutionProvider\")\n",
    "print(\"ml模型加载成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb4a26-5021-4a4f-99cc-b9d41cb1822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用ML-Danbooru模型\n",
    "image_path = \"/gemini/code/3.png\"\n",
    "result_text = main(image_path, onnx_model_path, ml_model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend)\n",
    "print(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5e2f4-c9d0-49f0-9046-269bc2120f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a051967-f6d4-4020-a863-8e16c0352d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载ShareGPT4V-13B\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "from flask import Flask, request, jsonify\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "\n",
    "os.environ[\"HF_OFFLINE\"] = \"1\"\n",
    "\n",
    "model_path = \"/models/ShareGPT4V-13B\"\n",
    "'''加载模型'''\n",
    "setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n",
    "setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, \"llava-v1.5-7b\", False, False)\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\") or file.endswith(\".webp\") or file.endswith(\".gif\"):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image_paths.append(image_path)\n",
    "                del image_path\n",
    "    return image_paths\n",
    "\n",
    "def get_responce(prompt_int, image_file, temperature=0.7, top_p=0.4, max_new_tokens=512, num_beams=1):\n",
    "    '''输入内容、图片来获取输出'''\n",
    "    prompt_use = \"A chat between a curious human and an artificial intelligence assistant, the content of the chat and it should be accurate. \\nUSER: <image>\\n\"\n",
    "    prompt = prompt_use + prompt_int + \"\\nASSISTANT:\"\n",
    "    image = Image.open(image_file)\n",
    "    images = [image.convert(\"RGB\")]\n",
    "    images_tensor = process_images(images, image_processor, model.config).to(model.device, dtype=torch.float16)\n",
    "    input_ids = (tokenizer_image_token(prompt, tokenizer, -200, return_tensors=\"pt\").unsqueeze(0).cuda())\n",
    "    keywords = [\"</s>\"]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(input_ids, images=images_tensor, do_sample=True if temperature > 0 else False, temperature=temperature, top_p=top_p, num_beams=num_beams, max_new_tokens=max_new_tokens, use_cache=True, stopping_criteria=[stopping_criteria])\n",
    "    input_token_len = input_ids.shape[1]\n",
    "    outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n",
    "    outputs = outputs.strip()\n",
    "    if outputs.endswith(\"</s>\"):\n",
    "        outputs = outputs[: -len(\"</s>\")]\n",
    "    del prompt_use,prompt,image,images,output_ids,input_token_len,images_tensor,input_ids,stopping_criteria\n",
    "    return outputs\n",
    "\n",
    "def process_text(outputs):\n",
    "    \"\"\"去除重复的句子并拼合成一整段\"\"\"\n",
    "    text = ' '.join(outputs.splitlines()).strip()\n",
    "    sentences = text.split('.')\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip() not in unique_sentences:\n",
    "            unique_sentences.append(sentence.strip())\n",
    "    final_text = '.'.join(unique_sentences)\n",
    "    del text,sentences,unique_sentences,outputs\n",
    "    return final_text\n",
    "\n",
    "def process_savetxt(output, image_file_path):\n",
    "    \"\"\"输入图片获取返回json\"\"\"\n",
    "    file_name = os.path.splitext(os.path.basename(image_file_path))[0]\n",
    "    txt_file_path = os.path.join(os.path.dirname(image_file_path), file_name + \".txt\")\n",
    "    with open(txt_file_path, 'w') as f:\n",
    "        f.write(process_text(output))\n",
    "    del output,file_name,txt_file_path,image_file_path\n",
    "\n",
    "def process_request(output):\n",
    "    response_json = {\"output\": output}\n",
    "    del output\n",
    "    return response_json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb20fb8-b0aa-4a8a-8bb0-cd09f6ba8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_int = \n",
    "temperature = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692ae50-85a0-4ea3-9342-90aa373787ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "@app.route('/api', methods=['POST'])\n",
    "def handle_request():\n",
    "    prompt_default = \"tell me about\"\n",
    "    image_save_path = \"/gemini/code/image\"\n",
    "    os.makedirs(image_save_path, exist_ok=True)\n",
    "    prompt_int = request.form.get('prompt_int', prompt_default)\n",
    "    image_file = request.files['image']\n",
    "    temperature = float(request.form.get('temperature', '0.7'))\n",
    "    top_p = float(request.form.get('top_p', '0.4'))\n",
    "    max_new_tokens = int(request.form.get('max_new_tokens', '512'))\n",
    "    num_beams = 1\n",
    "    if 'image' not in request.files:\n",
    "        return jsonify({'error': '你需要发送一个图片'})\n",
    "    image_file_dir = os.path.join(image_save_path, f\"{time.time()}_input.jpg\")\n",
    "    image_file.save(image_file_dir)\n",
    "    executor = concurrent.futures.ThreadPoolExecutor()\n",
    "    result_text = main(image_file_dir, onnx_model_path, ml_model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend)\n",
    "    prompt_int2 = \"The main character in the picture is \"+prompt_int+\", The name of the role must be mentioned in the answer. This is the result of marking through the marker:\"+result_text+\". For all tags, they must be used when describing the image (synonyms can be used). Then describe the picture.\"\n",
    "    print(prompt_int2)\n",
    "    \"\"\"\n",
    "    future2 = executor.submit(get_responce, prompt_int2, image_file_dir, temperature, top_p, max_new_tokens, num_beams)\n",
    "    response_json = process_request(future2.result())\n",
    "    \"\"\"\n",
    "    future = get_responce(prompt_int2, image_file_dir, temperature, top_p, max_new_tokens, num_beams)\n",
    "    print(future)\n",
    "    response_json = process_request(future)\n",
    "    return jsonify(response_json)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e9739-4770-4f76-b00b-377176b5394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_dir = \"/gemini/code/image/1703243535.1733773_input.jpg\"\n",
    "result_text = main(image_file_dir, onnx_model_path, model, threshold, size, keep_ratio, use_spaces, use_escape, include_ranks, score_descend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b54327-06de-4ad6-851c-79755dd283b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对单张图片进行处理，返回txt\n",
    "result = \"1girl, red_eyes, solo, official_alternate_costume, red_dress, silver_hair, white_background, dress, looking_at_viewer, long_sleeves, long_hair, simple_background, aqua_headwear, hat, breasts, bangs, nail_polish, parted_lips, medium_breasts, bare_shoulders, hair_over_one_eye, black_neckwear, off_shoulder, clothing_cutout, ascot, detached_sleeves, upper_body, black_headwear, holding, artist_name, hand_up, wide_sleeves, weibo_username, water, twitter_username, sitting\"\n",
    "prompt_int = \"This is the result of marking through the marker:\"+result+\". For all tags, they must be used when describing the image (synonyms can be used). Then describe the picture.\"\n",
    "#prompt_int = \"describe the picture.\"\n",
    "image_file_path = \"/gemini/code/3.png\"\n",
    "get_responce(prompt_int, image_file_path, temperature=0.7, top_p=0.4, max_new_tokens=512, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54100f0e-2bb4-4466-aaa0-7da486960a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  config.json  preprocessor_config.json  pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -a /root/.cache/huggingface/hub/models--Lin-Chen--ShareGPT4V-13B_Pretrained_vit-large336-l12/snapshots/738faab4f9b1f76d62408d5ea7f36d55f5e55464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187b7a3d-3c34-4807-a39b-7f339ca04da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /gemini/code/preprocessor_config.json /root/.cache/huggingface/hub/models--Lin-Chen--ShareGPT4V-13B_Pretrained_vit-large336-l12/snapshots/738faab4f9b1f76d62408d5ea7f36d55f5e55464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e0878-fec9-4827-bc48-b58b0c1eb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = get_image_paths(image_directory)\n",
    "output = get_responce(prompt_int, image_file_path, temperature=0.7, top_p=0.4, max_new_tokens=512, num_beams=1)\n",
    "#process_savetxt(output, image_file_path)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57055d0-a376-4037-8764-6aece8d9e482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
